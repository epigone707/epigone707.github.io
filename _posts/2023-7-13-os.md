---
layout: post
title: "Operating System Interview Note"
description: "It recaps the os basics and would be extremely helpful if you are going to take an interview tomorrow but forget what you learnt in college"
category: tech
tags: interview os
modify: 2023-07-13 18:09:00
---

# Intro

This is my note for os interview. It recaps the os basics and would be extremely helpful if you are going to take an interview tomorrow but forget what you learnt in college (that's why I make this note!).

reference:
- [uci jbell's coursenote](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems)

# 1 Process
## 1.1 Process Concept

A process is basically a program in execution. To put it in simple terms, we write our computer programs in a text file and when we execute this program, it becomes a process which performs all the tasks mentioned in the program.

When a program is loaded into the memory and it becomes a process, it can be divided into four sections: stack, heap, text and data.

- Stack: the temporary data such as method/function parameters, return address and local variables.
- Heap: dynamically allocated memory to a process during its run time.
- Text: the compiled program code, read in from non-volatile storage when the program is launched.
- Data: global and static variables, allocated and initialized prior to executing main.

![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter3/3_01_Process_Memory.jpg)

### Process State

Processes may be in one of 5 states:

- New - The process is in the stage of being created.
- Ready - The process has all the resources available that it needs to run, but the CPU is not currently working on this process's instructions.
- Running - The CPU is working on this process's instructions.
- Waiting - The process cannot run at the moment, because it is waiting for some resource to become available or for some event to occur. For example the process may be waiting for keyboard input, disk access request, inter-process messages, a timer to go off, or a child process to finish.
- Terminated - The process has completed.

![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter3/3_02_ProcessState.jpg)

### Process Control Block (PCB)

For each process there is a Process Control Block, which stores the following process-specific information:

- Process State - Running, waiting, etc.
- Process ID, and parent process ID.
- CPU registers - register set where process needs to be stored for execution for running state
- Program Counter (PC) - a pointer to the address of the next instruction to be executed for this process
- CPU Scheduling information - Such as priority information and pointers to scheduling queues.
- Memory Management information - page/segment table and memory limits
- Accounting information - user and kernel CPU time consumed, account numbers, limits, etc.
- I/O Status information - list of I/O devices allocated to the process.

![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter3/3_04_ProcessSwitch.jpg)

## 1.2 Process Scheduling

The two main objectives of the process scheduling system are to keep the CPU busy at all times and to deliver "acceptable" response times for all programs.

- All processes are stored in the **job queue**.
- Processes in the Ready state are placed in the **ready queue**.
- Processes waiting for a device to become available or to deliver data are placed in **device queues**. There is generally a separate device queue for each device.
- Other queues may also be created and used as needed.

### Context Switch

a context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point, and then restoring a different, previously saved, state.

WHEN?

Most commonly, one process must be switched out of the CPU so another process can run. This context switch can be triggered by the process making itself unrunnable, such as by waiting for an I/O or synchronization operation to complete.

On a pre-emptive multitasking system, the scheduler may also switch out processes that are still runnable. To prevent other processes from being starved of CPU time, pre-emptive schedulers often configure a timer interrupt to fire when a process exceeds its time slice. This interrupt ensures that the scheduler will gain control to perform a context switch.

Context switching happens VERY frequently

HOW?

In the Linux kernel, context switching involves loading the corresponding process control block (PCB) stored in the PCB table in the kernel stack to retrieve information about the state of the new process. CPU state information including the registers, stack pointer, and program counter as well as memory management information like segmentation tables and page tables (unless the old process shares the memory with the new) are loaded from the PCB for the new process.

## 1.3 Operations on Processes
### Process Creation
- Processes may create other processes through appropriate system calls, such as `fork` or `spawn`. The process which does the creating is termed the parent of the other process, which is termed its child.
- In the parent process, `fork()` returns the pid of the child. In the child process, `fork()` returns 0, which is not the pid of anything, it's just a marker.
- Each process is given an integer identifier, termed its process identifier, or **PID**. The parent PID ( PPID ) is also stored for each process.
- On typical UNIX systems, the process scheduler is termed **sched**, and is given PID 0. The first thing it does at system startup time is to launch **init**, which gives that process PID 1. Init then launches all system daemons and user logins, and becomes the ultimate parent of all other processes.

There are two options for the parent process after creating the child:
- Wait for the child process to terminate before proceeding. The parent makes a `wait( )` system call, for either a specific child or for any child, which causes the parent process to block until the `wait( )` returns. UNIX shells normally wait for their children to complete before issuing a new prompt.
- Run concurrently with the child, continuing to process without waiting. This is the operation seen when a UNIX shell runs a process as a background task. It is also possible for the parent to run for a while, and then wait for the child later, which might occur in a sort of a parallel processing operation. ( E.g. the parent may fork off a number of children without waiting for any of them, then do a little work of its own, and then wait for the children. )

Two possibilities for the address space of the child relative to the parent:
- The child may be an exact duplicate of the parent, sharing the same program and data segments in memory. Each will have their own PCB, including program counter, registers, and PID. This is the behavior of the `fork` system call in UNIX.
- The child process may have a new program loaded into its address space, with all new code and data segments. This is the behavior of the `spawn` system calls in Windows. UNIX systems implement this as a second step, using the `exec` system call.

Figure below shows the `fork` and `exec` process on a UNIX system. Note that the `fork` system call returns the PID of the processes child to each process - It returns a zero to the child process and a non-zero child PID to the parent, so the return value indicates which process is which. Process IDs can be looked up any time for the current process or its direct parent using the `getpid()` and `getppid()` system calls respectively.
![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter3/3_10_C_fork.jpg)
![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter3/3_10_ProcessCreation.jpg)

### Process Termination
Processes may request their own termination by making the `exit( )` system call, typically returning an int. This int is passed along to the parent if parent is doing a `wait( )`, and is typically zero on successful completion and some non-zero code in the event of problems.

Processes may also be terminated by the system for a variety of reasons, including:
- The inability of the system to deliver necessary system resources.
- In response to a KILL command, or other un handled process interrupt.
- A parent may kill its children if the task assigned to them is no longer needed.
- If the parent exits, the system may or may not allow the child to continue without a parent.

When a process terminates, all of its system resources are freed up, open files flushed and closed, etc. The process termination status and execution times are returned to the parent if the parent is waiting for the child to terminate, or eventually returned to init if the process becomes an orphan. 

A [**zombie process**](https://en.wikipedia.org/wiki/Zombie_process) is a process that has completed execution (via the `exit` system call) but still has an entry in the process table. The cause of zombie process: the parent is not waiting for child process, i.e., doesn't pick up the child's exit code.

An **orphan process** is a process that is still executing, but whose parent has died. When the parent dies, the orphaned child process is adopted by **init** (process ID 1). 

## 1.4 Inter Process Communication (IPC)

![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter3/3_12_CommunicationsModels.jpg)


_Communications models: (a) Message passing. (b) Shared memory._

### Shared Memory

- Multiple processes are given access to the same block of memory, which creates a shared buffer for the processes to communicate with each other.
- Shared memory is generally preferable when large amounts of information must be shared quickly on the same computer.

### Message Passing

- Message passing systems must support at a minimum system calls for "send message" and "receive message".
- Either the sending or receiving of messages ( or neither or both ) may be either blocking or non-blocking.
- Messages are passed via queues
- Message Passing requires system calls for every message transfer, and is therefore slower, but it is simpler to set up and works well across multiple computers. Message passing is generally preferable when the amount and/or frequency of data transfers is small, or when multiple computers are involved.

## 1.6 Communication in Client-Server Systems

### Sockets

- A socket is an endpoint for communication.
- Two processes communicating over a network often use a pair of connected sockets as a communication channel. Software that is designed for client-server operation may also use sockets for communication between two processes running on the same computer - For example the UI for a database program may communicate with the back-end database manager using sockets. ( If the program were developed this way from the beginning, it makes it very easy to port it from a single-computer system to a networked application. )
- A socket is identified by an IP address concatenated with a port number

Communication channels via sockets may be of one of two major forms:

- Connection-oriented ( TCP, Transmission Control Protocol ) connections emulate a telephone connection. All packets sent down the connection are guaranteed to arrive in good condition at the other end, and to be delivered to the receiving process in the order in which they were sent. The TCP layer of the network protocol takes steps to verify all packets sent, re-send packets if necessary, and arrange the received packets in the proper order before delivering them to the receiving process. There is a certain amount of overhead involved in this procedure, and if one packet is missing or delayed, then any packets which follow will have to wait until the errant packet is delivered before they can continue their journey.
- Connectionless ( UDP, User Datagram Protocol ) emulate individual telegrams. There is no guarantee that any particular packet will get through undamaged, and no guarantee that the packets will get delivered in any particular order. There may even be duplicate packets delivered, depending on how the intermediary connections are configured. UDP transmissions are much faster than TCP, but applications must implement their own error checking and recovery procedures.

Sockets are considered a low-level communications channel, and processes may often choose to use something at a higher level, such as those covered in the next two sections.

### Remote Procedure Calls (RPC)

The general concept of RPC is to make procedure calls similarly to calling on ordinary local procedures, except the procedure being called lies on a remote machine.

Implementation involves stubs on either end of the connection.

- The local process calls on the stub, much as it would call upon a local procedure.
- The RPC system packages up ( marshals ) the parameters to the procedure call, and transmits them to the remote system.
- On the remote side, the RPC daemon accepts the parameters and calls upon the appropriate remote procedure to perform the requested work.
- Any results to be returned are then packaged up and sent back by the RPC system to the local system, which then unpackages them and returns the results to the local calling procedure.

### Pipes

Pipes are one of the earliest and simplest channels of communications between ( UNIX ) processes.

# 2 Threads

## 2.1 Overview

- A thread is a basic unit of CPU utilization, consisting of a program counter, a stack, and a set of registers, ( and a thread ID. )
- Traditional ( heavyweight ) processes have a single thread of control - There is one program counter, and one sequence of instructions that can be carried out at any given time.
- As shown below, multi-threaded applications have multiple threads within a single process, each having their own program counter, stack and set of registers, but sharing common code, data, and certain structures such as open files.
![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter4/4_01_ThreadDiagram.jpg)

Example: web server - Multiple threads allow for multiple requests to be satisfied simultaneously, without having to service requests sequentially or to fork off separate processes for every incoming request. ( The latter is how this sort of thing was done before the concept of threads was developed. A daemon would listen at a port, fork off a child for every incoming request to be processed, and then go back to listening to the port. )

Benefits of multi-threading:
- Responsiveness - One thread may provide rapid response while other threads are blocked or slowed down doing intensive calculations.
- Resource sharing - By default threads share common code, data, and other resources, which allows multiple tasks to be performed simultaneously in a single address space.
- Economy - Creating and managing threads ( and context switches between them ) is much faster than performing the same tasks for processes.
- Scalability, i.e. Utilization of multiprocessor architectures - A single threaded process can only run on one CPU, no matter how many may be available, whereas the execution of a multi-threaded application may be split amongst available processors. ( Note that single threaded processes can still benefit from multi-processor architectures when there are multiple processes contending for the CPU, i.e. when the load average is above some certain threshold. )

## 2.2 Multicore Programming

A multi-threaded application running on a traditional single-core chip would have to interleave the threads, as shown below:
![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter4/4_03_ConcurrentSingleCore.jpg)

On a multi-core chip, however, the threads could be spread across the available cores, allowing true parallel processing, as shown below:
![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter4/4_04_ParralelMulticore.jpg)

## 2.3 Thread Pools

- Creating new threads every time one is needed and then deleting it when it is done can be inefficient, and can also lead to a very large ( unlimited ) number of threads being created.
- An alternative solution is to create a number of threads when the process first starts, and put those threads into a thread pool.
   - Threads are allocated from the pool as needed, and returned to the pool when no longer needed.
   - When no threads are available in the pool, the process may have to wait until one becomes available.
- The ( maximum ) number of threads available in a thread pool may be determined by adjustable parameters, possibly dynamically in response to changing system loads.
- Win32 provides thread pools through the "PoolFunction" function. Java also provides support for thread pools through the java.util.concurrent package

## 2.4 Others 
- UNIX provides two separate system calls, `kill( pid, signal )` and `pthread_kill( tid, signal )`, for delivering signals to processes or specific threads respectively.
- Linux does not distinguish between processes and threads - It uses the more generic term "tasks". The traditional `fork( )` system call completely duplicates a process ( task ), as described earlier.

# 3 Synchronization
## 3.1 Mutex
- Most systems offer a software API equivalent called mutexes. ( For mutual exclusion )
- The terminology when using mutexes is to acquire a lock prior to entering a critical section, and to release it when exiting, as shown below:
![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter5/5_08_Locks.jpg)
- The acquire step will block the process if the lock is in use by another process, and both the acquire and release operations are atomic.
- Acquire and release can be implemented as shown here, based on a boolean variable "available":

```c++
acquire() {
    while (!available); /* busy wait*/
    available = false;
}

release() {
    available = true;
}
```

- One problem with the implementation shown here, is the busy loop used to block processes in the acquire phase. These types of locks are referred to as **spinlocks**, because the CPU just sits and spins while blocking the process.
- Spinlocks are wasteful of cpu cycles, and are a really bad idea on single-cpu single-threaded machines, because the spinlock blocks the entire computer, and doesn't allow any other process to release the lock. ( Until the scheduler kicks the spinning process off of the cpu. )
- On the other hand, spinlocks do not incur the overhead of a context switch, so they are effectively used on multi-threaded machines when it is expected that the lock will be released after a short time.

## 3.2 Semaphore

- Semaphore is a more robust synchronization mechanism that is used to control access to shared resources.
- It maintains a integer count of available resources and provides two atomic operations: wait() and signal(). The counter allows it to control access to a finite pool of resources.

The following is an implementation of semaphore:

```c++
wait(S) {
   while (S<=0);
   S--;
}

signal(S) {
   S++;
}
```

- **Counting semaphore** can take on any integer value, and are usually used to count the number remaining of some limited resource.
- **Binary semaphore** is used if there is only one count of a resource. It can only have the values of 0 or 1. They are often used as mutex locks. Here is an implementation of mutual-exclusion using binary semaphores:

```c++
while (1)
{
    wait(s);
    /*
        critical section
    */
    signal(s);
    /* 
        remainder section
     */
}
```

- The problem of the implementation described above is the busy loop in the `wait` call, which consumes CPU cycles without doing any useful work. This type of lock is known as a **spinlock**. As we mentioned in mutex section, spinlock does have the advantage of not invoking context switches.
- An alternative approach is to block a process when it is forced to wait for an available semaphore, and swap it out of the CPU. In this implementation each semaphore needs to maintain a list of processes that are blocked waiting for it, so that one of the processes can be woken up and swapped back in when the semaphore becomes available. ( Whether it gets swapped back into the CPU immediately or whether it needs to hang out in the ready queue for a while is a scheduling problem. ) The new definition of a semaphore and the corresponding `wait` and `signal` operations are shown as follows:
![image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter5/5_Semaphore2.jpg)

Note that in this implementation, the value of the semaphore can become negative, in which case its magnitude is the number of processes waiting for that semaphore.

# Main-Memory Management

## Belady's Anomaly

Bélády's anomaly is the phenomenon in which increasing the number of page frames results in an increase in the number of page faults for certain memory access patterns. This phenomenon is commonly experienced when using the first-in first-out (FIFO) page replacement algorithm.

In FIFO, the page fault may or may not increase as the page frames increase, but in optimal and stack-based algorithms like LRU, as the page frames increase, the page fault decreases.

[Example from wikipedia](https://en.wikipedia.org/wiki/B%C3%A9l%C3%A1dy%27s_anomaly)


# Scheduling

## Deadlock and four necessary conditions

ref: [what-is-deadlock-and-what-are-its-four-necessary-conditions](https://afteracademy.com/blog/what-is-deadlock-and-what-are-its-four-necessary-conditions/)

Deadlock is a situation where a set of processes are blocked as each process is holding resources and waits to acquire resources held by another process. This leads to infinite waiting.

Necessary Conditions of Deadlock:

- Mutual Exclusion: A resource can be held by only one process at a time
- Hold and Wait: A process can hold a number of resources at a time and at the same time, it can request for other resources that are being held by some other process.
- No preemption: A resource can't be preempted from the process by another process, forcefully. For example, if a process P1 is using some resource R, then some other process P2 can't forcefully take that resource.
- Circular Wait: Circular wait is a condition when the first process is waiting for the resource held by the second process, the second process is waiting for the resource held by the third process, and so on. At last, the last process is waiting for the resource held by the first process.

Deadlock will happen if all the above four conditions happen simultaneously.


# Other

## Spooling

Spooling (Simultaneous Peripheral Operations Online) is an I/O management or buffer management technique that allows the data of the input/output processes to be temporarily stored in the secondary memory (Hard Drive, SSD, etc.) which will be executed by the CPU or a device or a program. These data will be stored in the secondary memory until the system or a program requests the data for its execution.
